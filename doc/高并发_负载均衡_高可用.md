# 网络高并发负载均衡高可用

### 计算机软件工程学的重点之一
* 分层解耦, 各司其职 (分工)
* 对任何一层进行调优或修改, 不会对上下2层有影响

## OSI 7层网络, tcp 4层网络
> 应用层, 表示层, 会话层, 传输控制层, 网络层, 链路层, 物理层

* 应用层
    * 和用户交互的软件, 浏览器, Tomcat, 应用软件
    * http协议
    * ssh协议
    * smtp协议
* 表示层
    * 协议语义, 段落划分, 字符串表示, 加密解密
* 会话层
    * session等等
* 传输控制层
    * 控制建立连接不连接, 控制传输成功失败等
    * 有TCP, UDP
* 网络层
    * 设备如何去路由, 如何找到网络节点
* 链路层
    * 点对点之间使用什么协议, 具体到数据写成什么样才能通信
    * 有p2p点对点, 也有以太网的
* 物理层
    * 4G, 光纤, WIFI等等网络设备



### 面向连接

* 客户端和服务端 资源创建和销毁的过程 叫做面向连接
  * 三次握手 -> 数据传输 -> 四次挥手  称为: **最小粒度** , 不可被分割, 原子性
* ***端点与端点的所有传输数据是不能被打散的***
  * 如S1, S2组成服务端的集群, Client1 与 S1已经有2次握手, 不论任何技术实现, 不应该将第三次握手路由到S2, 而应该路由到S1

```shell
netstat -natp
# n 是ip地址不要用逻辑名称显示, 不显示主机名/域名, 显示ip地址, numeric  dont't resolve names(host,port,user)
# a 是所有套接字(socket)
# t 是 tcp协议
# p 是pid, 显示通信是哪个进程号
```

```shell
zz@v001:~$ netstat -natp
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1124/sshd
tcp        0     52 192.168.1.24:22         192.168.1.6:11802       ESTABLISHED 1346/sshd: zz [priv
tcp6       0      0 :::22                   :::*                    LISTEN      1124/sshd

# 如最上面1124进程号是ssh的主进程, 因为有window客户端连接到ssh, 所以会产生一个副进程
# 套接字靠什么唯一区分出来, 本地IP:端口 + 远程IP:端口, 视为一个套接字socket
# 能区分的意思是, 各自传输的时候, IO是不互相影响的
```



### 使用文件描述符 fd来操作http传输

```shell
cd /proc/$$/fd

echo 8<> /dev/tcp/www.baidu.com/80 
echo -e 'GET / HTTP1.0\n' > & 8
cat < & 8

# 第一步建立连接
# 第二步才是传输数据, get请求 (http协议: 规范和标准)
# 第三步查看响应结果
# 这是一个应用层协议
```





### 网络配置文件知识

```shell
cat /etc/sysconfig/network-scripts/ifcfg-eth0

DEVICE=eth0
#HWADDR=00:0C:29:42:15:C2
TYPE=Ethernet
ONBOOT=yes
NM_CONTROLLED=yes
BOOTPROTO=static
IPADDR=192.168.150.14 # IP地址, 标识描述网络节点中的坐标
NETMASK=255.255.255.0 # 掩码,
GATEWAY=192.168.150.2 # 网关地址
DNS1=223.5.5.5 # 云解析地址1, 给DNS域名, DNS解析成IP地址
DNS2=114.114.114.114
```

* IP地址也叫 点分字节, 每个点儿隔开的是一个字节, 一个字节有8个二进制位, 0~255
* 掩码, IP地址和掩码会发生按位与(&)操作, 可以得出网络号
  * 如 192.168.150.14 & 255.255.255.0  得出  192.168.150.0, 这就是所在网络的网络号
  * 本机是 192.168.150.0 这个网络中的第14号



### 网络层 & 路由表 & 下一跳机制

```shell
route -n # 路由表
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.1.0     0.0.0.0         255.255.255.0   U     0      0        0 enp0s3 #网卡
0.0.0.0         192.168.1.1     0.0.0.0         UG    0      0        0 enp0s3

# 第一条记录在下面会被提及
```

* 想象如果没有下一跳机制, 一个端点想和另一个端点通信, 需要将整个图(数据结构) 记录到本地, 这样数据很大且计算很慢, 需要做图计算
* 下一跳机制是, 端点内存中无需存储所有的端点坐标(IP), 只需要记录我的出路在哪里即可
* 每个互联网的设备不需要存全网的数据, 只需要存它周边一步的数据(即自己网络号中的数据)



### 如何基于下一跳到百度(另一个网络的端点)

```shell
zz@v001:~$ ping www.baidu.com
PING www.a.shifen.com (61.135.169.125) 56(84) bytes of data.
64 bytes from 61.135.169.125 (61.135.169.125): icmp_seq=1 ttl=58 time=3.92 ms
64 bytes from 61.135.169.125 (61.135.169.125): icmp_seq=2 ttl=58 time=3.39 ms
```

* 先不讨论DNS解析域名为IP
* 当我们知道IP的时候, 用百度的IP  61.135.169.125 和 路由表中第一个条目的掩码 255.255.255.0 做一次位运算, 得出 61.135.169.0
  * 61.135.169.0 在路由表中 无法匹配 路由表中的 第一个条目: 192.168.1.0
  * 所以路由表中第一个条目被淘汰
  * 61.135.169.125 和 路由表中 下一个条目的掩码 0.0.0.0做与运算, 得出 0.0.0.0, 匹配路由表的destination
  * 此时得到下一跳地址: 192.168.1.1
  * 掩码为0.0.0.0的那条记录的GATEWAY, 称之为默认网关
  * 当 gateway的值为0.0.0.0说明destination和当前网络节点在同一局域网, 同一局域网不需要下一跳就可以通信, 所以网关地址是 0.0.0.0,  网与网之间才需要路由器

### 链路层 & 如何把包发给百度

* 问题: 把数据包从当前服务器发给百度(61.135.169.125), 目标地址应该填写百度的地址,还是网关的地址(192.168.1.1)
  * 应该是百度的IP地址, 因为如果发给 192.168.1.1, 那这台机器(192.168.1.1)一看地址是自己, 就不会再发给百度了
  * 那如何将61.135.169.125发给正确的下一跳呢? 答案:不是网络层该解决的问题, 需要下探到链路层

```shell
zz@v001:~$ sudo arp -a
? (192.168.1.6) at 70:8b:cd:bd:05:cc [ether] on enp0s3
_gateway (192.168.1.1) at 24:44:27:cd:17:17 [ether] on enp0s3


```

* DNS会解析域名与IP地址的映射, 作用于的全网的
* ARP会解释IP地址与网卡硬件地址的映射, 作用域是在同一局域网内的
* 最外层写给 下一跳的mac地址(24:44:27: cd:17:17), 内层写 发送给谁(百度), 最内层写端口号
* 最后通过这3层, 给端点, 并通过端口号确定端点的进程
* 再进行下一跳时, IP,端口都不会发生变化, 只有mac地址发生变化, 变化为下一跳的地址

### 结论

* TCP-IP协议是基于下一跳的机制
  * IP是端点间
  * mac地址是节点间的(下一跳)



### ARP协议

* arp是用于获取 IP地址与MAC地址的映射关系
* arp -a 可以采集到局域网的除了自己之外的其他IP地址和mac对应关系
* 但是一开机时是空的, 所以需要发送一个arp包
* 首次发送的arp包的mac地址很特殊, FFFFFFF,  目标IP是192.168.1.1
* 在源端点和路由器之间还有交换机, 交换机负责广播, 从自己的打开的端口广播出去
  * 如1号口接收arp, 2号口连接另一台计算机, 3号口连接路由器
  * 此时, 交换机广播, 通过2号口和3号口广播出去
  * 因为1号是接收的, 不会广播, 并且还会记录: 1号口接受过 源端点的IP和MAC地址
* 若此时网络中总计3个端点: 自己, 另一个计算机, 路由器(192.168.1.1), 如果另一台计算机收到arp包且地址为192.168.1.1, 因为目标不是自己, 则丢弃
* 而192.168.1.1这台端点发现目标是自己, 才会响应arp数据包
* 路由器会收录源mac地址和IP, 并把自己的mac地址封装成arp包, 目标地址是源地址, 然后发回去
* 回路经过交换机, 因为来时记录了端口对应的源mac地址, 所以不会再次广播, 而是从之前接受过的端口发出去(定点转发)



### 数据包的层次

1. 源端口号 -> 目标端口号	传输控制层	描述的是某一个端点中的进程, 具体哪个服务
2. 源IP地址 -> 目标IP地址	网络层	找到合适路径跳到目标节点上
3. 源MAC地址 -> 目标MAC地址	链路层	解决点与点之间下一条从哪儿来去哪儿

* 通过这3点, 完成路径的寻址



### Tomcat慢原因

* 是七层架构, 其他层走完才能走到他这块儿
* 他是一个应用程序, 操作系统要从内核态切换到用户态, 才能执行
* 自己本身又是建立在JVM上的, 相当于在用户态中有虚拟了一块儿内存





### 网络高并发负载均衡机选型

* 负载均衡不能和客户端建立握手, 而是透传过去, 与让用户与服务器建立握手
* 只是四层网络协议, 做数据包的转发
* 特征
  * 特别快
  * 数据包转发级别
  * 不会和客户端进行握手
  * 因为不会看数据包内容, 如URI, 所以要求后端服务器是镜像的, 就是一模一样的克隆机
  * 如果想知道数据内容, 如URI等, 则必须要和客户端有一次握手的过程
* 在企业中
  * Nginx并发是有上限的, 官方数据是5W个并发
  * LVS只要硬件足够快, 带宽足够大, 选择合适的组网模式, 能抗住的并发远大于Nginx
  * 一般是LVS在最前端, hold住流量
  * 在后面一堆Nginx, hold住握手
  * 收到请求后, Nginx再和后面计算层(如tomcat)握手

* 内核态 四层及以下, 以上用户态



### 负载模式升级过程, 无NAT, D-NAT(目标网络地址转换destination network address translation)

* 家里路由器上网是 S-NAT, source, 会改变源IP
  * 客户端发送请求到百度, CIP->BDIP
  * 路由器会改变源IP, 也就是将CIP->BDIP改成 RIP->BDIP, 并记录X端口与CIP->RIP的转换关系, 然后发给百度
  * 百度响应 BDIP->RIP
  * 路由器收到响应后, 查看有转换是CIP->RIP的X端口
  * 将  BDIP->RIP转换成 BDIP->CIP
  * 然后从X端口发给客户端
* 客户端CIP, LVS有2个网卡VIP, DIP
  * VIP用于接收客户端的CIP发送的数据
  * DIP用于给后端转发
  * 后端的叫 RIP( real IP)
* 此时客户端发送数据给VIP
  1. CIP -> VIP
  2. LVS通过DIP把数据包发给后端服务器, 后端服务器会丢弃
  3. 因为收到的数据包是 CIP->VIP, 跟我RIP没关系, 所以丢弃
* 此时LVS做D-NAT地址转换协议
  1. CIP -> VIP
  2. 在从DIP转发之前将VIP换成RIP
  3. CIP -> RIP
  4. 此时后端tomcat能接收并处理请求
  5. 但不能直接返回给CIP,  因为当前socket是 RIP -> CIP, 不是 VIP -> CIP, 客户端不是你是谁, 数据包被丢弃
  6. 此时只能转发给LVS, LVS再做一次返地址转换, 恢复成 VIP ->CIP
  7. 此过程不是我们想要的, 有弊端
     1. 因为通信是非对称的, 客户端发送的响应头的极小的, 服务器返回的数据是很大的(参考用fd给百度发送数据, 百度返回的数据量比发送的信息很大很多)
     2. 结合第1点贷宽成为瓶颈
     3. 消耗算力

### 要求服务器直接客户端返回, DR(直接路由模式)

* 要求后端服务器直接客户端返回
* 那么最终肯定是 VIP -> CIP 这样的数据包
* 如果后端服务器中有一个隐藏的VIP, 只有自己知道, 则当 CIP -> VIP时, 因为可知此VIP, 能接受并处理, 然后转换 VIP->CIP 给客户端返回
* 但是 对于LVS来说, 当前是  CIP -> VIP, 向后转发也是 CIP -> VIP, 是有问题的, 因为自己是VIP, 相当于自己在发给自己, 自己转圈玩
* 所以需要到链路层, 将VIP绑定上后端服务器RIP的MAC地址, 这样才能跳到服务器的端点上
* 二层网络协议, 也可称之为 MAC地址欺骗
* 后端服务器实现VIP隐藏
  * 修改内核, 内核不会暴露VIP出去, 对ARP协议做手术, 别人发ARP请求, 想获取VIP的MAC地址, 本机不响应
  * 导致除了自己外, 局域网内没人知道自己有VIP, 但自己肯定知道有VIP
* 后端服务器接可以收到请求, 可以处理请求, 然后反向 VIP->CIP 再发出去,直接发给客户端, 就实现了直接路由模式



### 如果不在一个局域网内, 需要用TUN隧道技术, 也就是VPN

* 客户端CIP, 负载均衡上是VIP和DIP, 后端sever是RIP
* 负载均衡在生成一个数据包, DIP->RIP
* 这个包, 包裹着上一个包   DIP->RIP(CIP->VIP), 这样会出发互联网的跳跃, 后端sever能收到包
* 后端server把外层包撕掉, 就知道是 CIP->VIP, 并建立映射关系socket
* 然后处理完, 就可以给CIP返回
* 隧道就是, IP的数据包背着(套着)一个IP的数据包



### 如何隐藏VIP

* 内核映射成文件, 存放在 /proc/sys/net/ipv4/conf/* IF */

* 定义收到ARP请求时的响应级别
  * arp_ignore:
    * 0: 只要本地配置的有相应地址, 就给予响应
    * 1: 仅在请求的目标(MAC)地址配置请求到达的接口上的时候, 才给予响应
* 定义将自己地址向外通告的通告级别
  * arp_annouce:
    * 0: 将本地任何接口上的任何地址向外通告
    * 1: 试图仅向目标网络通告与其网络匹配的地址
    * 2: 仅向与本地接口上地址匹配的网络进行通告
* 要想做DR需要
  * arp_ignore 设置为 1
  * arp_annouce 设置为 2







### 回环网卡

* Linux内核中的一块虚拟网卡
* 如架设tomcat端口8080, 并在**本地浏览器**访问localhost:8080, 过程如下
  * host解析localhost的IP地址为127.0.0.1
  * 请求交给内核
  * 内核发现虚拟网卡中有对应的127.0.0.1, **不走物理网卡**,  输入变成输出,  直接返回给**应用程序**tomcat
  * tomcat处理完请求, 将相应交给内核, 内核发现响应的IP地址与虚拟网卡中对应, 直接转发给本地的浏览器程序
  * 整个过程没有使用到物理网卡
* 所以即便内核接通电源, lo也不可能与外界通信, 因为没有物理输出
* 另外, 与外界通信, 外界只知道有物理网卡, 不知道内核的虚拟网卡, 也就是回环网卡
* lo也可以配子接口, 因为每块网卡可以配置多个子IP
* 所以我们把VIP配置在lo, 可以达到隐藏VIP的目的, 内核知道有, 外界不知道有
* 对外隐藏, 对内可见



### 调度算法

* 静态调度
  * rr: 轮询, 轮着来, 如有2台后端server分别是a和b, 第一次给a, 第二次给b, 第三次给a, 如此类推, 相当于计数器做哈希
  * wrr: 加权轮询
  * dh
  * sh
* 动态算法
  * lc: 最少链接
    * 问题: 负载均衡器不与客户端建立握手, 他如何知道后端server有多少个连接?
    * 答案是: 偷窥技术, 只看不动手
    * 客户端发送握手请求时, 数据包三次握手是有状态码识别的, 如***SYNC状态码***
    * 如要转给后端server1, 负载均衡器偷窥第一次握手的包SYNC, 以及偷窥了第三个握手的包ACK, 则记录后端server1的连接数 + 1
    * 但不能只增不减, 断开也会被偷窥, ***FIN状态***
    * 偷窥四次挥手的第一次和第四次, 则记录给server1的连接数 - 1
  * wlc: 加权最少链接
  * sed: 最短期望延迟
  * nq: never queue
  * LBLC: 基于本地最少连接
  * DH
  * LBLCR: 基于本地的带复制功能的最少连接



### LVS使用

* yum install ipvsadm -y

* 管理集群服务

  * 添加 -A -t|u|f service-address [-s scheduler]
    * -t: tcp协议
    * -u: udp协议
    * service-address: IP:PORT
    * -f: FWM: 防火墙标记
    * -s: 调度算法
    * service-address: Mark Number
  * 修改: -E
  * 删除: -D -t|u|f service-address
  * 如: ipvsadm -A -t 192.168.9.100:80 -s rr

* 管理集群服务中的RS

  * 添加: -a -t|u|f server-address -r server-address [-g|i|m] [-w weight]

    * -t|u|f server-address: 事前定义好的集群服务
    * -r server-address: 某RS的地址, 在NAT模型中, 可使用IP:PORT试下端口映射
    *  [ -g | i | m ]  : LVS类型
      * -g: DR模型
      * -i: TUN模型
      * -m: NAT
    * [-w weight] : 定义服务器权重

  * 修改: -e

  * 删除: -d -t|u|f service-address -r server-address

    * ```shell
      ipvsadm -a -t 172.16.100.1:80 -r 192.168.10.8 -g
      ipvsadm -a -t 172.16.100.1:80 -r 192.168.10.9 -g
      ```

  * 查看: -L|l

    * -n: 数字格式显示主机地址和端口
    * --stats: 统计数据
    * --rate: 速率
    * --timeout: 显示tcp, tcpfin和udp的会话超时时长
    * -c: 显示当前的ipvs连接状况

  * 删除所有集群的服务

    * -C: 清空ipvs规则C大写

    * ```shell
      # 保存规则
      # ipvsadm -S > /path/to/somefile
      ```

    * ```shell
      
      ```
  # 载入此前的规则
      # ipvsadm -R < /path/to/somefile
      ```





### 实战

1. 准备三台机器node01, node02, node03, 并把网络层布置好

   | 主机名 | IP             | 作用         | 布置操作                              |
   | ------ | -------------- | ------------ | ------------------------------------- |
   | node01 | 192.168.150.11 | 作为lvs      | 并在eth0网卡增加VIP 192.168.150.100   |
   | node02 | 192.168.150.12 | 作为server01 | 并在lo回环网卡增加VIP 192.168.150.100 |
   | node03 | 192.168.150.13 | 作为server02 | 并在lo回环网卡增加VIP 192.168.150.100 |

2. 先在node01上配置vip, 不要在node02和node03上配置, 因为这样就暴露出去了

   ```shell
   # 在node01上配置VIP
   $ ifconfig eth0:2 192.168.1.100/24
   # /24表示多少个1,3*8个1=24个1, 也就是255.255.255.0
   # /16就是表示255.255.0.0
   $ ifconfig eth0:2 down
   # 如果不想要了, 给他down掉
   ```

3. 调整node02和node03的arp协议

   ```sh
   cd /proc/sys/net/ipv4/conf/eth0
   cat arp_ignore # 查询当前值是多少
   0 # 返回值是0
   # CentOs 这里不要用vi来取修改, 用echo来重定向
   echo 1 > arp_ignore
   echo 2 > arp_announce
   # Ubuntu切换到root用户 sudo -s 或者 sudo -i  再执行
   
   # 此外还要把 all目录也修改成隐藏
   echo 1 > /proc/sys/net/ipv4/conf/all/arp_ignode
   echo 2 > /proc/sys/net/ipv4/conf/all/arp_announce
   
   # 记录绝对路径操作, 网卡需要用的时候换一下
   echo 1  >  /proc/sys/net/ipv4/conf/enp0s3/arp_ignore
   echo 1  >  /proc/sys/net/ipv4/conf/all/arp_ignore
   
   echo 2  >  /proc/sys/net/ipv4/conf/enp0s3/arp_announce
   echo 2  >  /proc/sys/net/ipv4/conf/all/arp_announce
   ```

4. 调整node02和node03的VIP

   ```shell
   $ ifconfig lo:1 192.168.1.100 netmask 255.255.255.255
   # 这里掩码要生成为4个255, 因为掩码和IP要发生一次按位与的计算, 来生成路由条目
   # 如果此时是3个255, 那么现在有2个网卡(包括虚拟)能到达 192.168.1.0这个地址
   # 此时如果ping 192.168.150.1, 因为虚拟网卡距离内核更近, 虚拟网卡会输入变输出, 包根本就发不出去
   # 也是规避死循环的风险
   ```

5. 使用ipvsadm命令开启负载均衡

   ```shell
   ipvsadm -C # 清空之前的配置
   ipvsadm -A  -t  192.168.1.100:80  -s rr # 新建一个负载均衡规则,只能代理相同port
   ipvsadm -a  -t 192.168.1.100:80  -r  192.168.1.26 -g
   ipvsadm -a  -t 192.168.1.100:80  -r  192.168.1.24 -g
   ipvsadm -ln # 查看当前配置
   
   # -A 指定Address也就是IP+PORT
   # -t 指的是tcp
   # -s scheduler, 负载策略
   # rr round robin 轮询
   
   # -a 增加后端server, 需要带着VIP+VPORT, 指定挂在那个负载配置下
   # -r 后端server的address,IP+VPORT
   # -g 指定DR直接路由模式
   ```

6. 建议使用curl来尝试, 因为自己用浏览器尝试时有会话保持, 没有达到轮询的效果

7. 



### 排查错误

1. 如果在 ipvsadm -ln里查看到, 有连接状态是 SYNC_RECV, 基本上lvs都记录了, lvs没事儿, 一定是后边网络层出问题了



### 高可用

* 问题

  1. LVS挂掉, 整个业务下线, 单点故障
  2. RS挂掉, 一部分用户请求异常, LVS还存有这个RS的负载记录

* 解决问题的思路

  * 单点故障的解决方式: 它是一个, 一个有问题, 我们就用一堆: 一变多

* 2个思路, 多点2种形式

  1. 主备模式
  2. 主主模式
  3. 主从模式

* 讨论主备的方向性,效率性, 如皇帝->皇子们

  * 皇子如何知道皇帝挂了
  * 方向性, 皇帝通知皇子们(广播), 还是皇子自己去查皇帝状态
  * 效率性, 皇子们如何决定谁接替主
    * 退让制 VS 争抢制度

  * 主备, 主(单点->主备) 从
  * RS挂了怎么确定, 你如何确定后baidu挂了
    * ping不对
    * 正确方式: 访问一下. 底层:验证应用层的http协议, 发请求, 判断响应码 200 ok

* 实现方式

  * LVS内核中有模块 ipvs, 修改内核源码是一种方式, 但不推荐
  * 第三方程序外挂实现, 人是最不靠谱的
  * 企业追求自动化, 把人解耦出去, 用程序替代 -> keepalived
  * 代替人做自动化运维, 解决单点故障, 实现HA
    1. 监控自己的服务
    2. Master通告自己还活着, Backup监听Master状态, Master挂了, 一堆Backup推举出一个新的Master
    3. 配置: vip, 添加ipvs, keepalived是有配置文件
    4. 对后端server做健康检查
  * keepalived 是一个通用工具, 主要作为HA的实现
  * nginx, 可以作为公司的负载均衡使用, 但是此nginx成为单点故障, 也可以用keepalived



### 会话保持概念

* 负载均衡机给同一个客户端请求路由到同一台服务上, 有助于资源的复用
* 如后台RS1接收了CIP1的请求, 开辟了如内存空间和数据库连接
* CIP1下一次请求, 若能再次负载到RS1上, 则能达到资源复用的目的
* 若负载到RS2上, RS2要重新开辟资源, 相当于资源浪费
* 实现环境中, 会话保持调整成为0秒, 方便观察负载均衡漂移过程
* 生产环境下, 必须要调整这个参数为N秒



### keepalived 配置

* 路径在 /etc/keepalived.conf
* 日志在 /var/log/syslog
  * 问题1: Configuration file '/etc/keepalived/keepalived.conf' is not a regular non-executable file
    * 解决 : chmod 644 /etc/keepalived/keepalived.conf
  * 问题2: Unknown keyword 'virtual_server'
    * 解决: 层次嵌套错了, virtual_server是根节点, 不是其他根节点的子节点
* 实验1: 测试LVS集群(主备)高可用 
  * 将主的网卡如eth0给down掉, 此时VIP漂移到备机上去
  * 此时如果主修复, 是否还能抢回主的位置
    * 因为LVS的主备之间没有太多数据需要同步
    * 所以主挂掉修复后可以抢主的位置
    * 其他主备模型未必是如此, 如大数据的主备模型, 需要如半小时同步, 此时down的主再回来就导致服务可不用, 所以不会再抢回主的位置
* 实验2: 抛砖引玉, 主机上杀死keepAlived进程
  * 会导致主机没有及时收回 VIP和负载的内核配置, 此时备机收不到主还活着, 所以也会出现VIP和内核配置
  * 守护进程down掉, 导致两台机器上都有VIP, 三次握手四次挥手就有可能建立不起来, 因为原子性破坏了, 第一次握手发给s1, 第三次发给s2, 从而出现丢包
  * 需要通过高可用集群来解决问题, 如zookeeper

```shell
! Configuration File for keepalived

global_defs {
    notification_email {
        acassen@firewall.loc
        failover@firewall.loc
        sysadmin@firewall.loc
    }
    notification_email_from Alexandre.Cassen@firewall.loc
    smtp_server 192.168.200.1
    smtp_connect_timeout 30
    router_id LVS_DEVEL
}


vrrp_instance VI_1 { # 虚拟路由冗余协议
    state MASTER     # 主备的 主模式
    interface enp0s3 # 把物理网络再做分割, 指定走那一块网卡
    virtual_router_id 51 # 与备机的id要一致, 表示做的是同一件事
    priority 100         # 权重值, 备机可以调整比重, 50太子, 30小皇子, 主挂掉, 50太子会优先接替
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }

    virtual_ipaddress { # 这就是VIP
        192.168.1.100/24 dev enp0s3 label enp0s3:3
    }
}

virtual_server 192.168.1.100 80 { # ipvsadm的-A,不能光有进来的还得有出去的,就是下面的real_server
    delay_loop 5 # 每个5秒检查一次real_server状态
    lb_algo rr   # LVS算法轮询
    lb_kind DR   # LVS模式直接路由
    nat_mask 255.255.255.0
    persistence_timeout 0     # 会话保持时间秒数(实验环境改成0,不保持会话,生产根据情况调整,且必须调整)
    protocol TCP

    real_server 192.168.1.24 80 {
        weight 1
        HTTP_GET {
            url {
                path /
                status_code 200
            }
            connect_timeout 5    # 连接超时时间
            nb_get_retry 2       # 重连次数
            delay_before_retry 3 # 重连间隔时间
        }
    }
    real_server 192.168.1.26 80 {
        weight 1
        HTTP_GET {
            url {
                path /
                status_code 200
            }
            connect_timeout 5
            nb_get_retry 2
            delay_before_retry 3
        }
    }
}

```





高频(重点关注)
1.多线程
2.jvm
3,设计模式
4.zookeeper
5.redis
6.mysql 调优


每天整理, 每周汇总

3个小时/天
1.5小时看
1小时整理笔记, 如果整理不完, 说明至少有1个技术点有遗漏,再取补单项

3-4月试水
9-10月再试水



