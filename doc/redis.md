# Redis

### 常识

* 磁盘的维度
  1. 寻址的速度: ms毫秒级别的
  2. 带宽: 单位时间有多少个字节流动, GB/MB 级别
* 内存
  * 寻址的速度: ns 纳秒 (秒>毫秒>微秒>纳秒)
    * 硬盘在寻址上比内存慢了10万倍
  * 带宽: 很大
* I/O buffer 成本问题
  * 磁盘有磁道和扇区
    * 一扇区 512Byte字节
    * 如果一个区域足够小, 带来的问题是成本变大->索引
    * 格式化磁盘时有4K对齐, 真正使用硬件时并不是512B为读写量
    * 操作系统, 无论读取多少都是最少4K, 从磁盘上拿
  * 随着文件变大, 读取速度变慢
    * 硬盘成为瓶颈, 也就是I/O成为瓶颈
* 数据库的出现
  * data page 概念
    * 大小为4K,  与操作系统一次IO相同, 放一行一行的数据, 
    * 相当于数据块, 并由软件统一管理, 使得其变得更线性, 而不是散落在磁盘各处
      * 如果数据库定义小了, 则亏得慌, 如定义1k, 但每次IO是4K, 有3k没有发挥作用
      * 可以往大了调整
      * 个人理解: Hadoop的数据块概念就可以很大
  * 索引
    * 也是基于4K的数据块
    * 记录并指向data page的地址
    * 存储于硬盘
  * 关系型数据建表: 必须给出schema
    * 类型: 字节宽度
    * 存储: 倾向于行级别的存储, 即使一行除了主键其他字段无数据, 也会给这些字段开辟空间
      * 站位的好处: 为将来增删改提供扩展性, 降低成本
  * 使用索引需要内存中准备一个B+ Tree
    * 所有叶子节点为硬盘上的索引
    * 树干(非叶子节点) 存储于内存之中
  * 命中缓存的流程
    * where条件命中内存中的B+树的节点
    * 也就是命中磁盘中索引块
    * 数据库加载磁盘索引块到内存, 解析得出数据块地址
    * 通过数据块地址将数据块内容加载到内存, 并返回给用户
    * 最终目的减少IO的次数
  * 问题: 如果数据库的表数据量很大, 性能下降, 查询效率会变低
    * 如果表有索引, 增删改变慢
      * 原因, 修改完数据, 还会修改索引, 会调整索引的位置
    * 查询速度
      1. 数据量很大前提下(假设达到物理极限), 1个或者少量查询, 且命中索引的情况下, 依然很快
      2. 并发或复杂SQL到达, 要获取多个data page, 会受硬盘带宽影响速度
    * 解决方案
      1. 使用昂贵的基于内存的关系型数据库
      2. 使用缓存做为中间件
  * 数据在磁盘和内存体积不一样, 因为磁盘中没有对象概念, 引用需要冗余数据
    * 将2T磁盘数据, 转换成内存的数据结构, 要<2T
    * 但是基于内存的关系型数据库很贵, 贵到买不起
* 问题: 数据库数据增长很大很快, 又不可能使用基于内存的关系型数据库, 如何操作?
  * 折中方案: 缓存, 如memcached, redis
* 2个基础设施
  1. 冯诺依曼体系的硬件
  2. 以太网, TCP/IP网络(潜台词: 不稳定)

* **技术选型**&**技术对比**能力
  * [数据库引擎网](https://db-engines.com/en/)    包括了关系型,内存kv型,文档型等等
  * 基于此网站来做数据库的选型

* memcached VS redis

  * 第一反应, memcached即使没有复杂数据类型, 也可以用json表示
    * 世界上有3种数据表示:
      * k = a
      * k = [1,2,3]
      * k = { x = y },  k = [{}, {}]

  * 数据结构
    * memcached: 他的value没有类型的概念
    * redis: 他的value有类型概念, 如string, list等等
    * 这不是重点, 因为可以用json代替
    * 但是, 如果client要从缓存中读取value中的一个元素, 成本就不一样了
      * memcached: 返回所有value到client, 涉及到server,网卡,IO,解码json
      * redis: 类型不是很重要, 重要的是redis的server对每种类型都有自己的方法
      * 这样就规避的了上面的问题, 使用**命令**来代替**编码的实现**, 也实现了解耦
      * **计算向数据移动**

### 源码安装redis

* 安装过程
  1. yum install wget
  2. cd ~  &&  mkdir soft  &&   cd soft
  3. wget redis的下载地址
  4. tar xf redisXX.tar.gz # 这里没有v也是为了规避IO
  5. cd redis-src
  6. 看README.md
  7. make
  8. yum install gcc
  9. make distclean # 因为之间make过, 所以再次make时删除之前make过的内容
  10. make
  11. cd src # 查看, 此时生成了可执行程序
  12. cd.. #出来
  13. make install PREFIX=/opt/mashibing/redis
  14. vi /etc/profile
  15. export REDIS_HOME=/opt/mashibing/redis5
  16. export PATH=$PATH:$REDIS_HOME/bin
  17. source /etc/profile
  18. cd utils
  19. ./install_server.sh # 可执行一次或多次

* 知识点

  * 一个Linux操作系统(不论是物理机还是虚拟机) 可以有多个redis实例(进程), 通过端口号区分
  * 可执行程序只有一份, 但是内存中未来的多个实例, 需要各自的配置文件和持久化目录等资源
  * service redis_6379 start|stop|status  >  linux  /etc/init.d/redis_6379这个脚本

  

### redis特性

* 单进程, 单线程, 单实例, 一秒能hold住很多并发的请求, 那么他是如何变得很快的?
  * 客户端连接, 先到达内核, tcp握手
  * redis和内核之间使用的是 epoll, 非阻塞多路复用技术, 也是内核提供的一种系统调用
  * redis可以通过epoll系统调用, 来遍历不同客户端连接, 谁有数据就处理谁
  * 因为单进程, 所有数据到达并被redis处理是"有顺序的", 后面的共享空间的链表
* epoll
  * BIO时期
    * 当内核下客户端client连接过来, fd 8, fd 9
    * 操作系统提供一种读操作, read命令,  现在有线程/进程read fd8, 和另一线程/进程read fd9
    * 此时socket在这个时期是阻塞的, 也就是read命令阻塞不能返回, 等待client 对fd 8做写入操作
    * 如果现在, fd 9有数据写入, CPU时间片上只能有一个线程处理, 如正在处理fd8的read, 此时间片内还轮不到fd 9处理
    * 这样会导致, 当client增多时, 需要抛出更多的线程, client没有写入数据的时候, 很多线程会阻塞
    * CPU并没有处理真正到达的数据, 资源浪费, 且切换线程是有成本的
  * NIO时期(同步非阻塞时期), 为了提高硬件利用率, 内核发生变化
    * socket fd可以是 nonblock, 通过查询 man read 得出, read 命令有 nonblock选项
    * 此时不阻塞了, CPU可以只跑一个进程, 这个线程里写一个死循环
    * 调用完 read fd 8, 就调用read fd 9
    * 如果fd8没有数据, fd9有数据, 跳过fd8, 处理fd9
    * 这个轮询发生在用户空间(用户态)
    * 同步非阻塞体现在, 调用read方法后可以继续往下走不阻塞, 读fd的数据还是这个唯一的线程来做, 这叫同步
    * 问题: 如果有1000个fd, 代表**用户进程**轮询调用1000次**内核**, 成本很大
      * 也就是用户态和内核态切换, CPU保护现场恢复现场等资源消耗, 成本消耗
      * 想减少系统调用, 用户自己无法实现, 需要内核向前发展
  * select 多路复用的NIO
    * 把用户态的轮询放到了内核里, 内核里多了一个系统调用: select
    * 用户态调用select传入1000个fd, 内核监控这些fd, 内核等待直到有一个或多个I/O操作处于ready状态后返回, 参考: man select
    * 用户态再拿着返回的文件描述符, 再去调用read命令
    * 相当于还是调用wait , 但是不会调用没有ready, 没有数据写入的fd
    * 与上一个时期相比, 系统调用更精准了
    * 还是有问题: 每次要传入很多个fd进去, 返回后再去调用read, 是否还能再优化?
      * 抽象成: 用户态和内核态来回拷贝数据, 能否零拷贝
      * 决策环节时, 文件描述符fd成为累赘了
  * epoll 伪AIO
    * mmap命令, map or unmap files or devices into memory
    * 内核有内核的内存地址空间, 应用进程有应用进程内存地址空间, 都是虚拟地址空间, 物理内存上就是2个不同的区域而已, 内核的区域进程是不能直接访问的, 要通过API传参访问, 也就是拷贝来拷贝去
    * 索性弄出一个内核态和用户态都可以访问的空间, 称之为共享空间
    * 共享空间是通过内核的mmap系统调用实现的
    * 有了共享空间, 用户态就不用在自己的空间里写fd了, 要不然还得再拷贝一次到共享空间
    * 空箱空间是用户态内存空间的一部分, 也是内核态内存空间的一部分, 各自无需再单独维护fd
    * 空间中存放了 红黑树和链表 数据结构
      * 进程里(用户态)每多了一个fd, 就放到共享空间里的红黑树
      * 内核获取fd, 通过所有的IO/中断, 把数据达到的(ready)放到链表里
      * 当链表里有数据了, 用户态通过读取链表读取到fd, 再调用read读取数据
    * 零拷贝, sendfile
      * 现在有内核, 网卡, 文件, 应用程序
      * 文件fd3 -> 内核buffer -> read(fd3) -> 应用程序 -> write(fd4) -> 内核buffer -> 网卡请求
      * 有了sendfile之后
      * 文件fd3 -> 内核buffer -> read(fd3) -> 应用程序 -> sendfile(fd4) -> 网卡请求
    * sendfile + mmap 就有了一新的技术叫: kafka
      * 生产者: 网卡 -> kafka -> mmap -> 文件
      * 消费者: 网卡 -> kafka ->  文件 -> sendfile -> socket



### 简单关联一下Nginx

* 要满足多少个CPU, 启动多少个worker进程
* 一个worker就可以把数据压到CPU的一二三级缓存了
* 每个worker使用的是epoll, 同步非阻塞机制下的多路复用
* 只有Windows有AIO, Linux没有AIO





阿萨德

